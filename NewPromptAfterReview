# ============================================

# .copilot-instructions

# Universal Instructions for API Test Generation (Playwright + Cucumber + JavaScript)

# ============================================

## üß≠ Objective

When the user requests API test generation, **read the INPUT_TYPE and INPUT_PATH from the `.env` file** to locate the API specification (Swagger/OpenAPI), then generate a **complete Playwright + Cucumber API test suite in JavaScript/TypeScript** for all endpoints defined in that specification.

**Configuration Source:**
* Read `.env` file for `INPUT_TYPE` and `INPUT_PATH`
* `INPUT_TYPE` can be: `swagger_json`, `swagger_url`, `openapi_json`, `openapi_url`
* `INPUT_PATH` specifies the location (file path or URL)

Assume:
* Node.js v18+ (ESM syntax enabled)
* Playwright v1.45+ (for `APIRequestContext`)
* Cucumber.js v10+ (BDD test framework)
* Chai v5+ (assertions)
* Ajv v8+ (JSON Schema validation)
* Allure reporter (allure-cucumberjs, allure-commandline)
* dotenv (environment variable management)
* The API specification provides valid request/response schemas

---

## üß© Code Architecture

The generated project must follow this structure:

```
features/
‚îú‚îÄ‚îÄ step_definitions/
‚îÇ   ‚îú‚îÄ‚îÄ <endpoint1>_steps.js      # Step definitions for endpoint1
‚îÇ   ‚îî‚îÄ‚îÄ <endpoint2>_steps.js      # Step definitions for endpoint2
‚îú‚îÄ‚îÄ support/
‚îÇ   ‚îú‚îÄ‚îÄ world.js                  # Playwright context initialization
‚îÇ   ‚îî‚îÄ‚îÄ hooks.js                  # Setup and teardown hooks
‚îú‚îÄ‚îÄ <endpoint1>.feature           # Gherkin scenarios for endpoint1
‚îî‚îÄ‚îÄ <endpoint2>.feature           # Gherkin scenarios for endpoint2
helpers/
‚îú‚îÄ‚îÄ <endpoint1>ApiHelper.js       # API helper for endpoint1
‚îú‚îÄ‚îÄ <endpoint2>ApiHelper.js       # API helper for endpoint2
‚îú‚îÄ‚îÄ retryHelper.js                # Retry logic utilities
‚îî‚îÄ‚îÄ schemaValidator.js            # JSON Schema validation (Ajv)
models/
‚îú‚îÄ‚îÄ <endpoint1>Models.js          # Data models for endpoint1
‚îî‚îÄ‚îÄ <endpoint2>Models.js          # Data models for endpoint2
reports/                          # Generated test reports
‚îú‚îÄ‚îÄ cucumber-report.html
‚îî‚îÄ‚îÄ cucumber-report.json
allure-results/                   # Allure test results (generated)
allure-report/                    # Allure HTML report (generated)
.env                              # Environment configuration
.env.example                      # Environment template
.gitignore                        # Git ignore rules
cucumber.js                       # Cucumber configuration
playwright.config.js              # Playwright configuration
package.json                      # Dependencies and scripts
TEST_SUMMARY.md                   # Test execution summary (generated)
README.md                         # Project documentation
```

**Naming Rules**

* Use lowercase filenames and camelCase function names.
* Examples:
  * `store_steps.js`
  * `storeApiHelper.js`
  * `retryHelper.js`
  * Functions: `createOrder()`, `getOrderById()`, `deleteOrder()`

---

## ‚öôÔ∏è Functional Requirements

### 1. Generate POJOs (Data Models)

* Read the API specification from the location specified in `.env` (`INPUT_TYPE` and `INPUT_PATH`)
* Based on schema definitions in the specification, generate data models for the endpoints
* Use **JavaScript classes** to represent request and response objects
* Include constructors and default values for required fields

### 2. Create Feature File (.feature)

* For each endpoint, generate:
   - A **distinct feature file** named after the endpoint, for example:
       * `store.feature` for `/store` endpoint
       * `order.feature` for `/order` endpoint
* Written in **Gherkin syntax** (`Given`, `When`, `Then`).
* Each endpoint operation (`POST`, `GET`, `DELETE`, `PUT` if available) should include:
  * Valid scenario (happy path)
  * Invalid scenario (error handling)

### 2.1 Conditional Negative Test Scenarios Based on HTTP Methods

* **If the API specification provides specific negative status codes**, implement those directly.
* **If negative status codes are not defined in the specification**, generate negative test cases based on common expectations for each HTTP method:

| Method | Allowed Negative Codes |
| ------ | ---------------------- |
| GET    | 401, 404, 500          |
| POST   | 400, 401, 404, 500     |
| PUT    | 400, 404, 500          |
| DELETE | 400, 404, 500          |
---

### 3. Create Step Definitions

* Implement using Playwright's `request.newContext()` API.
* For each endpoint, generate a separate step definition file that contains steps specific to that endpoint.
  * For example, if the endpoint is /store, create features/step_definitions/store_steps.js.
  * Ensure each step definition file includes only the relevant steps for that endpoint, avoiding any cross-contamination between different API endpoints.
* Steps must:
  * Perform HTTP requests using endpoint data from the API specification
  * Use `chai.expect` for response assertions **based strictly on the specification definitions**
  * Reuse helper methods for each operation
* **CRITICAL ASSERTION RULES:**
  * **Positive scenarios (200/201/204):** MUST use status codes defined in the specification. NEVER modify assertions based on actual API behavior.
  * **Negative scenarios:** Use expected status codes (400, 401, 404, 500) and let tests fail naturally if API behavior differs.
  * **Do NOT implement "graceful handling"** that skips assertions when API returns unexpected codes.
  * Tests should FAIL when API behavior doesn't match expectations - this identifies real issues.

### 4. Create Helper Functions

* Each endpoint has a helper file, e.g., `helpers/storeApiHelper.js`.
* Each helper exports methods like `createOrder()`, `getOrder()`, `deleteOrder()`.
* Use Playwright's request context to make calls and return parsed JSON responses.

### 5. Schema Validation

**Purpose:** Validate response payloads against API specification schemas to ensure API contract compliance

**Scope - When to Apply Schema Validation:**

* **GET requests (200):** ALWAYS validate response schema
* **POST requests (200/201):** Validate IF response returns an object with the created resource
  - Skip if response is simple message like `{ "success": true }`
* **PUT requests (200/201):** Validate IF response returns an object with the updated resource
  - Skip if response is simple confirmation message
* **DELETE requests:** Typically no validation (204 No Content or simple confirmation)

**What to Validate:**

* Required fields are present in response
* Data types match specification (string, number, boolean, array, object)
* Nested objects structure matches schema
* Array items conform to expected schema

**Implementation Requirements:**

* Create `helpers/schemaValidator.js` using **Ajv** library for JSON Schema validation
* Extract schemas from API specification (Swagger/OpenAPI) for each endpoint and method
* Integrate validation in step definitions after successful responses
* Return detailed validation errors when schema doesn't match
* Add dependencies to `package.json`: `ajv@^8.12.0`, `ajv-formats@^2.1.1`

**Failure Handling:**

* Schema validation failures should be documented in TEST_SUMMARY.md under a new section:
  - **Schema Validation Failures:** Response structure doesn't match specification
  - Include: Endpoint, Expected schema, Actual response, Validation errors
  - Mark as CRITICAL issues (API contract violation)

### 6. Custom World & Hooks

* `features/support/world.js`: Initializes Playwright's request context.
* `features/support/hooks.js`: Adds setup and teardown hooks (`Before`, `After`).

### 7. Retry Logic

* Configure Cucumber to retry failed scenarios 3 times
* **Purpose:** Handle transient environment issues (network glitches, temporary service issues)
* **Implementation:** Add `retry: 3` to `cucumber.js` configuration
* **Behavior:**
  - Test runs and fails ‚Üí automatically retries up to 3 times
  - If passes on retry ‚Üí marks as passed (was transient issue)
  - If fails all 3 retries ‚Üí marks as FAILED (genuine API issue)
* **No code changes needed during retry** - assertions stay the same
* Document consistently failing tests in TEST_SUMMARY.md

Add to `cucumber.js`:
```javascript
export default {
  default: {
    retry: 3, // Retry failed scenarios 3 times
    // ... other config
  }
};
```

### 8. Environment Configuration

**Required Environment Variables (`.env` file):**

```env
# API Configuration
BASE_URL=https://api.example.com

# Authentication Configuration
# Supported AUTH_TYPE values: basic, bearer, none
AUTH_TYPE=basic

# Basic Authentication (when AUTH_TYPE=basic)
AUTH_USERNAME=your_username
AUTH_PASSWORD=your_password

# Bearer Token Authentication (when AUTH_TYPE=bearer)
# AUTH_TOKEN=your_bearer_token_here

# API Specification Configuration
INPUT_TYPE=swagger_json
INPUT_PATH=./api/spec.json
```

**Configuration Variables:**
* `BASE_URL` - API base URL endpoint
* `AUTH_TYPE` - Authentication method (`basic`, `bearer`, `none`)
* `AUTH_USERNAME` / `AUTH_PASSWORD` - For Basic Authentication
* `AUTH_TOKEN` - For Bearer Token Authentication
* `INPUT_TYPE` - API specification type (`swagger_json`, `swagger_url`, `openapi_json`, `openapi_url`)
* `INPUT_PATH` - Path to API specification (relative or absolute)

### 9. Config & Execution

* `playwright.config.js`: Sets baseURL from environment variable
* Read API specification location from `.env` (`INPUT_TYPE` and `INPUT_PATH`)
* `package.json`:

  ```json
  {
    "type": "module",
    "scripts": {
      "test": "cucumber-js --require 'features/**/*.js' --publish-quiet",
      "test:watch": "nodemon --exec \"npm test\"",
      "allure:report": "allure generate allure-results --clean -o allure-report",
      "allure:open": "allure open allure-report",
      "test:allure": "npm test && npm run allure:report"
    }
  }
  ```

### 10. Reporting

**Generate multiple report formats:**

1. **Cucumber HTML/JSON Reports:**
   * `/reports/cucumber-report.html`
   * `/reports/cucumber-report.json`

2. **Allure Report (Primary):**
   * Configure Allure reporter in `cucumber.js`:
     ```javascript
     format: [
       'progress-bar',
       'html:reports/cucumber-report.html',
       'json:reports/cucumber-report.json',
       'allure-cucumberjs/reporter'
     ],
     formatOptions: {
       resultsDir: './allure-results'
     }
     ```
   * Generate Allure report after test execution
   * Store results in `/allure-results/`
   * Generate HTML report in `/allure-report/`

3. **TEST_SUMMARY.md:**
   * Generate comprehensive markdown summary with:
     - Test Results: Total, Passed, Failed counts with success rate
     - Test Coverage by Endpoint: List all scenarios with ‚úÖ/‚ùå/‚ö†Ô∏è status
     - **Failed Test Cases Section:**
       * **Positive Scenarios:** Swagger-defined 200 responses that failed
       * **Negative Scenarios:** Tests with wrong error codes
       * **Schema Validation Failures:** Response structure mismatches
     - **Not Applicable Test Cases:** Tests failing due to missing API validation
     - API Issues Requiring Attention: Categorize as Critical vs Behavior Issues
     - Execution Details: Steps, duration, timestamps
     - Next Steps: Prioritized action items

**Dependencies & Configuration:**

* Add to `package.json` devDependencies: `allure-cucumberjs@^3.4.2`, `allure-commandline@^2.24.0`
* Add npm scripts (see Section 9 for complete package.json example)

**Report Features:**
* Include step-level logs and error stack traces
* Display execution summary in console output
* Allure dashboard with test statistics, trends, and categorization
* Interactive HTML reports with drill-down capabilities

---

## üß† Test Execution & Failure Handling

### Execution Rules

1. **Run all tests** after generation
2. **Do NOT modify test assertions** based on API responses
3. **Let tests fail naturally** when API behavior doesn't match Swagger or expectations
4. **Document all failures** in TEST_SUMMARY.md with clear categorization

### Failure Analysis & Categorization

When tests fail, categorize them in TEST_SUMMARY.md:

#### Category 1: Positive Scenario Failures (CRITICAL)
- **Definition:** Tests for successful operations (200/201/204) defined in Swagger
- **Handling:** Mark as FAILED in test report
- **Documentation:** List in "Failed Test Cases ‚Üí Positive Scenarios" section
- **Action:** These indicate API bugs - do NOT modify assertions
- **Example:** POST endpoint expects 200 per Swagger but returns 500

#### Category 2: Negative Scenario Failures - Validation Gaps
- **Definition:** Negative tests generated from common expectations (not in Swagger)
- **When API returns 200 instead of 400:**
  * Mark as FAILED in test report
  * Document in "Not Applicable Test Cases" section
  * Label as validation gap (API accepts invalid input)
  * Reason: API lacks input validation
- **Example:** Missing required parameter returns 200 instead of 400

#### Category 3: Negative Scenario Failures - Wrong Error Code
- **Definition:** Negative tests where API returns wrong error code
- **When API returns 500 instead of 400/404:**
  * Mark as FAILED in test report
  * Document in "Failed Test Cases ‚Üí Negative Scenarios" section
  * Reason: API has poor error handling
- **Example:** Invalid payload returns 500 instead of 400

#### Category 4: Schema Validation Failures (CRITICAL)
- **Definition:** Response structure doesn't match API specification schema
- **When schema validation fails:**
  * Mark as FAILED in test report
  * Document in "Failed Test Cases ‚Üí Schema Validation Failures" section
  * Label as API contract violation
  * Include detailed validation errors (missing fields, wrong types, etc.)
- **Example:** 
  * Missing required field in response
  * Field has wrong data type (string instead of number)
  * Response contains unexpected fields not in schema

### Retry Logic

* **Simple Scenario Retry:** Cucumber automatically retries failed scenarios 3 times
* **When test passes after retry:** Environment/transient issue - test passes
* **When test fails all 3 retries:** Genuine API issue - mark as FAILED and document in TEST_SUMMARY.md
* **No code modification during retries** - keep original assertions intact

### CRITICAL: What NOT to Do

‚ùå **NEVER** modify assertions to match actual API behavior  
‚ùå **NEVER** add graceful handling that skips failed assertions  
‚ùå **NEVER** change expected status codes based on API responses  
‚ùå **NEVER** add "# Note:" comments in feature files  
‚ùå **NEVER** mark tests as passing when they should fail
‚ùå **NEVER** skip schema validation when it fails

‚úÖ **ALWAYS** let tests fail when API behavior is wrong  
‚úÖ **ALWAYS** stick to Swagger definitions for positive scenarios  
‚úÖ **ALWAYS** document failures comprehensively in TEST_SUMMARY.md  
‚úÖ **ALWAYS** categorize failures correctly (Critical vs Validation Gaps vs Schema Violations)
‚úÖ **ALWAYS** perform schema validation on GET responses (200)
‚úÖ **ALWAYS** perform schema validation on POST/PUT responses IF they return an object (not simple message)
---

### **TEST_SUMMARY.md Generation Template**

Generate a comprehensive TEST_SUMMARY.md with this exact structure:

```markdown
# Test Execution Summary

## üìä Test Results
**Date:** [Current Date]
**Total Scenarios:** X
**Passed:** Y ‚úÖ
**Failed:** Z ‚ùå
**Success Rate:** Y/X (percentage)

## üß™ Test Coverage by Endpoint
[For each endpoint, list scenarios with ‚úÖ/‚ùå/‚ö†Ô∏è status]

## ‚ùå Failed Test Cases

### Positive Scenarios (Expected per Swagger)
[List scenarios where Swagger-defined success responses failed]
- Endpoint, Expected, Actual, Issue description

### Negative Scenarios (Generated based on common expectations)
[List negative test failures with wrong error codes]
- Test type, Expected, Actual, Reason

### Schema Validation Failures
[List scenarios where response structure doesn't match specification]
- Endpoint, Expected schema, Actual response, Validation errors, Fields affected

## ‚ö†Ô∏è Not Applicable Test Cases
[Detailed explanation for tests failing due to missing API validation]
- What was tested, Expected behavior, Actual behavior, Note

## üìù API Issues Requiring Attention
### Critical Issues (Positive Scenarios Failing + Schema Violations)
### API Behavior Issues (Validation Gaps)

## üéØ Test Execution Details
## ‚ú® Test Features
## üöÄ Next Steps
```

---

## üõ†Ô∏è **Authentication Handling**:

When generating API tests,
- Handle Basic authentication, Bearer Token, or API Key as defined in Swagger.  
- Encode Basic Auth as `Authorization: Basic <base64>`.
- Ensure that authentication is included for every API call requiring authentication, especially for endpoints that involve user data or secure operations.
---

## üåê **JSON Parsing Rules**

When parsing Swagger/OpenAPI **JSON** files:
* Accept **JSON file format** (`swagger.json` or `openapi.json`).
* Extract the following information:
  * Host and basePath
  * All available `paths` and their operations (GET/POST/PUT/DELETE)
  * Request/response schema from `definitions` or `components/schemas`
  * Example payloads if present
* Use these definitions to auto-generate request classes, payload builders, and assertions.

---

## üß™ Test Generation Workflow

When the user provides a **Swagger URL** or **Swagger JSON file** and specifies one or more API endpoints:
1. **Parse the Swagger definition** to extract all available endpoints.
2. Identify the specified endpoint(s) and their operations.
3. Generate   
   - **Data models** (JavaScript classes) 
   - **Gherkin feature files**
   - **Step definitions**
   - **Helper functions** 
   - **Hooks** and **world.js** files
4. Automatically create all required files in the current project. 
5. Run the tests
6. Apply Cucumber retry logic (3 attempts per scenario) for transient failures. Consistently failing scenarios are marked as FAILED.
7. Produce a summary and store results in `/reports/`.

---

## üßæ README.md Generation

Automatically include:
* Project overview
* Setup instructions (`npm install`, `npm test`)
* Folder structure explanation
* Swagger endpoint customization guide
* Adding new endpoint test instructions

---

## ‚úÖ Completion Criteria

All generated test scripts must:
* Execute using `npm test` (tests may fail - this is expected and correct)
* Validate response codes against Swagger definitions strictly
* Validate response schemas (required fields + data types) for successful responses
* Log meaningful console output with clear failure reasons
* Produce Cucumber HTML and JSON reports showing actual pass/fail status
* Generate comprehensive TEST_SUMMARY.md documenting all failures
* **NEVER modify assertions to make tests pass artificially**
* Be executable for any Swagger endpoint by providing Swagger URL/file and endpoint name
* Clearly distinguish between:
  - Critical failures (positive scenarios per Swagger + schema violations)
  - Validation gap failures (negative tests returning 200)
  - Error handling failures (negative tests returning wrong error codes)

---
## üó£Ô∏è **Prompt Schema (How to Interpret User Prompts)**

When the user requests API test generation, follow this workflow:

1. **Read Configuration from `.env` file**:
   * Read `INPUT_TYPE` to determine specification format (`swagger_json`, `swagger_url`, `openapi_json`, `openapi_url`)
   * Read `INPUT_PATH` to locate the specification file or URL
   * Read `BASE_URL` for the API endpoint
   * Read `AUTH_TYPE` to determine authentication method (`basic`, `bearer`, `none`)
   * Read authentication credentials based on AUTH_TYPE:
     - If `AUTH_TYPE=basic`: Read `AUTH_USERNAME` and `AUTH_PASSWORD`
     - If `AUTH_TYPE=bearer`: Read `AUTH_TOKEN`
     - If `AUTH_TYPE=none`: No authentication required
   
2. **Parse the API Specification**:
   * If `INPUT_TYPE` is `swagger_url` or `openapi_url`, fetch the specification from the URL in `INPUT_PATH`
   * If `INPUT_TYPE` is `swagger_json` or `openapi_json`, read the specification from the file path in `INPUT_PATH`
   * Ensure the specification follows Swagger 2.0 or OpenAPI 3.0 format
   * Extract all endpoints, operations, request/response schemas, and authentication requirements
   
3. **Generate the test suite**:
   * Generate a **complete Playwright + Cucumber API test suite** for all endpoints defined in the specification
   * Follow the architecture and rules outlined in this document
   * Generate separate feature files and step definitions for each endpoint
   
4. **Execute tests**:
   * Automatically execute the generated tests
   * Allow tests to fail naturally when API behavior doesn't match specification expectations
   * Do NOT retry assertion failures or modify expectations
   * Generate comprehensive TEST_SUMMARY.md documenting all results
   * Categorize failures as: Critical (specification violations), Validation Gaps (missing input validation), or Error Handling Issues (wrong error codes)

---



### üìò **User Prompt ‚Äî Generate API Regression Tests**

Generate Playwright + Cucumber API test scripts for all endpoints defined in the API specification configured in the `.env` file.
Please strictly follow the architectural and coding standards defined in the `.copilot-instructions` file at the project root.

---

### **Generation Goal**
* Read API specification from the location configured in `.env` (`INPUT_TYPE` and `INPUT_PATH`)
* Read authentication configuration from `.env` (`AUTH_TYPE`, credentials)
* Generate all necessary files (features, step definitions, API helpers, models, schema validator, config)
* Implement schema validation for response payloads (required fields + data types)
* Ensure tests execute successfully without manual intervention
* Tech Stack: Playwright (APIRequestContext) + Cucumber.js + Chai + Ajv (schema validation) + Allure

---

### **Expected Test Coverage**

For **each API operation**, generate the following tests:

1. **Positive Flow (200/201/204)**
   * Validate the request and response for successful operations
   * Assertions must strictly follow Swagger/OpenAPI definitions
   * **Perform schema validation** on successful responses:
     - GET (200): ALWAYS validate response schema
     - POST/PUT (200/201): Validate IF response returns an object (not simple message)
   * If test fails, it indicates an API bug (not a test bug)

2. **Negative Flows**
   * Generate **negative test cases** based on the applicable HTTP methods. The negative test cases should cover the following status codes:

   | HTTP Method | Allowed Negative Status Codes |
   | ----------- | ----------------------------- |
   | **GET**     | 401, 404, 500                 |
   | **POST**    | 400, 401, 404, 500            |
   | **PUT**     | 400, 404, 500                 |
   | **DELETE**  | 400, 404, 500                 |

   * **IMPORTANT:** Negative tests may fail if:
     - API lacks input validation (returns 200 instead of 400) ‚Üí Document as "Not Applicable"
     - API returns wrong error code (returns 500 instead of 400/404) ‚Üí Document as "Negative Scenario Failure"
   * Do NOT modify assertions to match actual API behavior
   * Assertions must reflect the EXPECTED behavior per standard HTTP conventions
   * If Swagger defines specific negative status codes, use those; otherwise use the table above

---

### **Authentication Handling**

* Read `AUTH_TYPE` from `.env` file (`basic`, `bearer`, or `none`)
* For `AUTH_TYPE=basic`: Use `AUTH_USERNAME` and `AUTH_PASSWORD`
* For `AUTH_TYPE=bearer`: Use `AUTH_TOKEN`
* For `AUTH_TYPE=none`: No authentication headers
* Apply authentication to all requests requiring it per Swagger security definitions

---

### **Schema Validation**

* Generate `helpers/schemaValidator.js` using Ajv library
* Extract schemas from Swagger/OpenAPI specification for each endpoint
* Validate response structure against schemas:
  - Required fields are present
  - Data types match (string, number, boolean, array, object)
  - Nested objects structure is correct
* Document schema validation failures as CRITICAL issues in TEST_SUMMARY.md

---

### **Execution Behavior**

* Generate tests that strictly follow the Swagger/OpenAPI definition
* Configure Cucumber retry logic (3 attempts per scenario) for transient failures
* Run tests after generation to validate execution
* **DO NOT modify assertions** to make tests pass
* Document all failures in TEST_SUMMARY.md with proper categorization:
  - **Positive Scenario Failures (CRITICAL):** Swagger-defined success responses that fail
  - **Negative Scenario Failures:** Tests with wrong error codes (500 instead of 400/404)
  - **Schema Validation Failures (CRITICAL):** Response structure doesn't match specification
  - **Not Applicable Test Cases:** Tests failing due to missing API validation (returns 200 instead of 400)
* If tests fail, it indicates **API issues**, not test issues

---

### **Reporting**

Generate the following reports after test execution:

1. **Cucumber HTML/JSON Reports** ‚Üí `/reports/cucumber-report.html`, `/reports/cucumber-report.json`
2. **Allure Report (Primary)** ‚Üí Generate interactive HTML dashboard with test statistics, trends, and drill-down capabilities
3. **TEST_SUMMARY.md** ‚Üí Comprehensive markdown summary with:
   - Test Results (Total, Passed, Failed, Success Rate)
   - Test Coverage by Endpoint (‚úÖ/‚ùå/‚ö†Ô∏è status)
   - Failed Test Cases (categorized by type)
   - Not Applicable Test Cases (validation gaps)
   - API Issues Requiring Attention (Critical vs Behavior Issues)
   - Next Steps (prioritized action items)

---

### ‚úÖ **Completion Criteria**

All generated test suites must:

* Include realistic positive and negative test coverage
* Implement schema validation for successful responses (GET always, POST/PUT conditionally)
* Execute using `npm test` with automatic retry (3 attempts) for transient failures
* Produce multiple report formats: Cucumber HTML/JSON, Allure dashboard, TEST_SUMMARY.md
* Assertions must match Swagger/OpenAPI definitions (not actual API behavior)
* Generate valid, executable test suites with proper authentication handling
* Document test failures properly with clear categorization (not hide them)
* Pass rate may be < 100% due to API bugs (this is expected and correct)
* Conform fully to the `.copilot-instructions` standards
